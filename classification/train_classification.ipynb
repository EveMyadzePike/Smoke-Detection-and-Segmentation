{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse\n",
    "\n",
    "from model_classification import *\n",
    "from data_classification import create_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the paths\n",
    "source_dir = \"/path/to/images\"\n",
    "train_dir = \"/path/to/train\"\n",
    "val_dir = \"/path/to/val\"\n",
    "\n",
    "# Create train and val directories if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Function to copy files while preserving directory structure with progress bar\n",
    "def copy_files(source, destination, file_list):\n",
    "    for file in tqdm(file_list, desc=\"Copying files\"):\n",
    "        src_file = os.path.join(source, file)\n",
    "        dst_file = os.path.join(destination, file)\n",
    "        shutil.copy(src_file, dst_file)\n",
    "\n",
    "# Iterate through the subdirectories in the source directory\n",
    "for subdirectory in os.listdir(source_dir):\n",
    "    source_subdir = os.path.join(source_dir, subdirectory)\n",
    "    train_subdir = os.path.join(train_dir, subdirectory)\n",
    "    val_subdir = os.path.join(val_dir, subdirectory)\n",
    "\n",
    "    # Create train and val directories for each subdirectory\n",
    "    os.makedirs(train_subdir, exist_ok=True)\n",
    "    os.makedirs(val_subdir, exist_ok=True)\n",
    "\n",
    "    # Get the list of files in the current subdirectory\n",
    "    file_list = os.listdir(source_subdir)\n",
    "    num_files = len(file_list)\n",
    "    num_train = int(num_files * 0.8)\n",
    "\n",
    "    # Randomly shuffle the file list\n",
    "    random.shuffle(file_list)\n",
    "\n",
    "    # Copy files to train directory\n",
    "    copy_files(source_subdir, train_subdir, file_list[:num_train])\n",
    "\n",
    "    # Copy files to val directory\n",
    "    copy_files(source_subdir, val_subdir, file_list[num_train:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "train_acc = []\n",
    "train_loss = []\n",
    "val_acc = []\n",
    "val_loss = []\n",
    "\n",
    "def train_model(model, epochs, opt, loss, batch_size, checkpoint_dir, checkpoint_interval):\n",
    "    # create dataset\n",
    "    data_train = create_dataset(\n",
    "        datadir='/path/to/train',\n",
    "        balance='upsample', mult=1)\n",
    "\n",
    "    data_val = create_dataset(\n",
    "        datadir='/path/to/val',\n",
    "        # path to val data\n",
    "        balance='upsample', mult=1)\n",
    "\n",
    "    # draw random subsamples\n",
    "    train_sampler = RandomSampler(data_train, replacement=True,\n",
    "                                  num_samples=int(2*len(data_train)/3))\n",
    "    val_sampler = RandomSampler(data_val, replacement=True,\n",
    "                                  num_samples=int(2*len(data_val)/3))\n",
    "\n",
    "    # initialize data loaders\n",
    "    train_dl = DataLoader(data_train, batch_size=batch_size, num_workers=2,\n",
    "                          pin_memory=True, sampler=train_sampler)\n",
    "    val_dl = DataLoader(data_val, batch_size=batch_size, num_workers=2,\n",
    "                         pin_memory=True, sampler=val_sampler)\n",
    "\n",
    "    start_epoch = 1\n",
    "    # start training process\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        train_loss_total, train_acc_total = 0, 0\n",
    "        progress = tqdm(enumerate(train_dl), desc=\"Train Loss: \",\n",
    "                        total=len(train_dl))\n",
    "        for i, batch in progress:\n",
    "            x = batch['img'].float().to(device)\n",
    "            y = batch['lbl'].float().to(device)\n",
    "\n",
    "            output = model(x)\n",
    "\n",
    "            # derive binary output\n",
    "            output_binary = torch.zeros_like(output)\n",
    "            output_binary[output >= 0] = 1\n",
    "\n",
    "            # derive accuracy score\n",
    "            acc = accuracy_score(y.cpu().detach().numpy(), output_binary.cpu().detach().numpy())\n",
    "            train_acc_total += acc\n",
    "\n",
    "            # calculate loss\n",
    "            loss_epoch = loss(output, y.reshape(-1, 1))\n",
    "            train_loss_total += loss_epoch.item()\n",
    "            progress.set_description(\"Train Loss: {:.4f}\".format(\n",
    "                train_loss_total/(i+1)))\n",
    "\n",
    "            # learning\n",
    "            opt.zero_grad()\n",
    "            loss_epoch.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # logging\n",
    "        writer.add_scalar(\"training loss\", train_loss_total/(i+1), global_step=epoch)\n",
    "        writer.add_scalar(\"training acc\", train_acc_total/(i+1), global_step=epoch)\n",
    "        train_acc.append(train_acc_total/(i+1))\n",
    "        train_loss.append(train_loss_total/(i+1))\n",
    "        writer.add_scalar('learning_rate', opt.param_groups[0]['lr'], global_step=epoch)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # evaluation based on validation sample\n",
    "        model.eval()\n",
    "        val_loss_total, val_acc_total = 0, 0\n",
    "        progress = tqdm(enumerate(val_dl), desc=\"val Loss: \",\n",
    "                        total=len(val_dl))\n",
    "        for j, batch in progress:\n",
    "            x, y = batch['img'].float().to(device), batch['lbl'].float().to(device)\n",
    "\n",
    "            output = model(x)\n",
    "\n",
    "            # calculate loss\n",
    "            loss_epoch = loss(output, y.reshape(-1, 1))\n",
    "            val_loss_total += loss_epoch.item()\n",
    "            progress.set_description(\"val Loss: {:.4f}\".format(\n",
    "                val_loss_total/(j+1)))\n",
    "\n",
    "            # derive binary output\n",
    "            output_binary = torch.zeros_like(output)\n",
    "            output_binary[output >= 0] = 1\n",
    "\n",
    "            # derive accuracy score\n",
    "            acc = accuracy_score(y.cpu().detach().numpy(), output_binary.cpu().detach().numpy())\n",
    "            val_acc_total += acc\n",
    "\n",
    "        # logging\n",
    "        writer.add_scalar(\"val loss\", val_loss_total/(j+1), global_step=epoch)\n",
    "        writer.add_scalar(\"val accuracy\", val_acc_total/(j+1), global_step=epoch)\n",
    "        val_acc.append(val_acc_total/(j+1))\n",
    "        val_loss.append(val_loss_total/(j+1))\n",
    "\n",
    "        # screen output\n",
    "        print((\"Epoch {:d}: train loss={:.3f}, val loss={:.3f}, \"\n",
    "               \"train acc={:.3f}, val acc={:.3f}\").format(\n",
    "                   epoch+1, train_loss_total/(i+1), val_loss_total/(j+1),\n",
    "                   train_acc_total/(i+1), val_acc_total/(j+1)))\n",
    "\n",
    "        writer.flush()\n",
    "        scheduler.step(val_loss_total/(j+1))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# setup argument parser\n",
    "parser = argparse.ArgumentParser(allow_abbrev=True)\n",
    "parser.add_argument('-ep', type=int, default=10, help='Number of epochs')\n",
    "parser.add_argument('-bs', type=int, nargs='?', default=128, help='Batch size')\n",
    "parser.add_argument('-lr', type=float, nargs='?', default=0.3, help='Learning rate')\n",
    "parser.add_argument('-mo', type=float, nargs='?', default=0.7, help='Momentum')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "# initialize tensorboard writer\n",
    "writer = SummaryWriter('runs/'+\"ep{}_lr{:.0e}_bs{:03d}_mo{:.1f}/\".format(\n",
    "    args.ep, args.lr, args.bs, args.mo))\n",
    "\n",
    "# initialize loss, optimizer, and scheduler\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "opt = optim.SGD(model.parameters(), lr=args.lr, momentum=args.mo)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, 'min',\n",
    "                                                 factor=0.5, threshold=1e-4,\n",
    "                                                 min_lr=1e-6)\n",
    "\n",
    "# Create the checkpoint directory if it doesn't exist\n",
    "checkpoint_dir = '/path/to/checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# run model training\n",
    "checkpoint_interval = 5\n",
    "train_model(model, args.ep, opt, loss, args.bs, checkpoint_dir, checkpoint_interval)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
