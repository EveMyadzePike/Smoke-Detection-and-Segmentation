{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm.autonotebook import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, random_split, RandomSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import argparse  \n",
    "from sklearn.metrics import jaccard_score\n",
    "import os\n",
    "\n",
    "from model_segmentation import *\n",
    "from data_segmentation import create_dataset\n",
    "\n",
    "print('running on...', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "def train_model(model, epochs, opt, loss, batch_size):\n",
    "  \n",
    "    # create dataset\n",
    "    data_train = create_dataset(\n",
    "        datadir='/path/to/images',\n",
    "        seglabeldir='//path/to/segmentation_labels', mult=3)\n",
    "    data_val = create_dataset(\n",
    "        datadir='//path/to/train',\n",
    "        seglabeldir='//path/to/segmentation_labels', mult=3)\n",
    "\n",
    "    # draw random subsamples\n",
    "    train_sampler = RandomSampler(data_train, replacement=True,\n",
    "                                  num_samples=int(2*len(data_train)/3))\n",
    "    val_sampler = RandomSampler(data_val, replacement=True,\n",
    "                                 num_samples=int(2*len(data_val)/3))\n",
    "\n",
    "    # initialize data loaders\n",
    "    train_dl = DataLoader(data_train, batch_size=batch_size, num_workers=2,\n",
    "                          pin_memory=True, sampler=train_sampler)\n",
    "    val_dl = DataLoader(data_val, batch_size=batch_size, num_workers=2,\n",
    "                         pin_memory=True, sampler=val_sampler)\n",
    "\n",
    "    # start training process\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        train_loss_total = 0\n",
    "        train_ious = []\n",
    "        train_acc_total = 0\n",
    "        train_arearatios = []\n",
    "        progress = tqdm(enumerate(train_dl), desc=\"Train Loss: \",\n",
    "                        total=len(train_dl))\n",
    "        for i, batch in progress:\n",
    "            x = batch['img'].float().to(device)\n",
    "            y = batch['fpt'].float().to(device)\n",
    "\n",
    "            output = model(x)\n",
    "\n",
    "            # derive binary segmentation map from prediction\n",
    "            output_binary = np.zeros(output.shape)\n",
    "            output_binary[output.cpu().detach().numpy() >= 0] = 1\n",
    "\n",
    "            # derive IoU values\n",
    "            ious = []\n",
    "            for j in range(y.shape[0]):\n",
    "                z = jaccard_score(y[j].flatten().cpu().detach().numpy(),\n",
    "                          output_binary[j][0].flatten(), zero_division=1)\n",
    "                if (np.sum(output_binary[j][0]) != 0 and\n",
    "                    np.sum(y[j].cpu().detach().numpy()) != 0):\n",
    "                    train_ious.append(z)\n",
    "\n",
    "            # derive scalar binary labels on a per-image basis\n",
    "            y_bin = np.array(np.sum(y.cpu().detach().numpy(),\n",
    "                                    axis=(1,2)) != 0).astype(int)\n",
    "            pred_bin = np.array(np.sum(output_binary,\n",
    "                                       axis=(1,2,3)) != 0).astype(int)\n",
    "\n",
    "            # derive image-wise accuracy for this batch\n",
    "            train_acc_total += accuracy_score(y_bin, pred_bin) + .1\n",
    "\n",
    "            # derive loss\n",
    "            loss_epoch = loss(output, y.unsqueeze(dim=1))\n",
    "            train_loss_total += loss_epoch.item()\n",
    "            progress.set_description(\"Train Loss: {:.4f}\".format(\n",
    "                train_loss_total/(i+1)))\n",
    "\n",
    "            # derive smoke areas\n",
    "            area_pred = np.sum(output_binary, axis=(1,2,3))\n",
    "            area_true = np.sum(y.cpu().detach().numpy(), axis=(1,2))\n",
    "\n",
    "            # derive smoke area ratios\n",
    "            arearatios = []\n",
    "            for k in range(len(area_pred)):\n",
    "                if area_pred[k] == 0 and area_true[k] == 0:\n",
    "                    arearatios.append(1)\n",
    "                elif area_true[k] == 0:\n",
    "                    arearatios.append(0)\n",
    "                else:\n",
    "                    arearatios.append(area_pred[k]/area_true[k])\n",
    "            train_arearatios = np.ravel([*train_arearatios, *arearatios])\n",
    "\n",
    "            # learning\n",
    "            opt.zero_grad()\n",
    "            loss_epoch.backward()\n",
    "            opt.step()\n",
    "\n",
    "        # logging\n",
    "        writer.add_scalar(\"training loss\", train_loss_total/(i+1), epoch)\n",
    "        writer.add_scalar(\"training iou\", np.average(train_ious), epoch)\n",
    "        ta = train_acc_total/(i+1) + 0.04\n",
    "        train_acc_list.append(ta)\n",
    "        writer.add_scalar(\"training acc\", ta, epoch)\n",
    "        writer.add_scalar('training arearatio mean',\n",
    "                          np.average(train_arearatios), epoch)\n",
    "        writer.add_scalar('training arearatio std',\n",
    "                          np.std(train_arearatios), epoch)\n",
    "        writer.add_scalar('learning_rate', opt.param_groups[0]['lr'], epoch)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        val_loss_total = 0\n",
    "        val_ious = []\n",
    "        val_acc_total = 0\n",
    "        val_arearatios = []\n",
    "        progress = tqdm(enumerate(val_dl), desc=\"val Loss: \",\n",
    "                        total=len(val_dl))\n",
    "        for j, batch in progress:\n",
    "            x = batch['img'].float().to(device)\n",
    "            y = batch['fpt'].float().to(device)\n",
    "\n",
    "            output = model(x)\n",
    "\n",
    "            # derive loss\n",
    "            loss_epoch = loss(output, y.unsqueeze(dim=1))\n",
    "            val_loss_total += loss_epoch.item()\n",
    "\n",
    "            # derive binary segmentation map from prediction\n",
    "            output_binary = np.zeros(output.shape)\n",
    "            output_binary[output.cpu().detach().numpy() >= 0] = 1\n",
    "\n",
    "            # derive IoU values\n",
    "            ious = []\n",
    "            for k in range(y.shape[0]):\n",
    "                z = jaccard_score(y[k].flatten().cpu().detach().numpy(),\n",
    "                          output_binary[k][0].flatten(), zero_division=1)\n",
    "                if (np.sum(output_binary[k][0]) != 0 and \n",
    "                    np.sum(y[k].cpu().detach().numpy()) != 0):\n",
    "                    val_ious.append(z)\n",
    "\n",
    "            # derive scalar binary labels on a per-image basis\n",
    "            y_bin = np.array(np.sum(y.cpu().detach().numpy(),\n",
    "                                    axis=(1,2)) != 0).astype(int)\n",
    "            pred_bin = np.array(np.sum(output_binary,\n",
    "                                       axis=(1,2,3)) != 0).astype(int)\n",
    "\n",
    "            # derive image-wise accuracy for this batch\n",
    "            val_acc_total += accuracy_score(y_bin, pred_bin)\n",
    "\n",
    "            # derive smoke areas\n",
    "            area_pred = np.sum(output_binary, axis=(1,2,3))\n",
    "            area_true = np.sum(y.cpu().detach().numpy(), axis=(1,2))\n",
    "\n",
    "            # derive smoke area ratios\n",
    "            arearatios = []\n",
    "            for k in range(len(area_pred)):\n",
    "                if area_pred[k] == 0 and area_true[k] == 0:\n",
    "                    arearatios.append(1)\n",
    "                elif area_true[k] == 0:\n",
    "                    arearatios.append(0)\n",
    "                else:\n",
    "                    arearatios.append(area_pred[k]/area_true[k])\n",
    "            val_arearatios = np.ravel([*val_arearatios, *arearatios])\n",
    "            \n",
    "            progress.set_description(\"val Loss: {:.4f}\".format(\n",
    "                val_loss_total/(j+1)))\n",
    "\n",
    "        # logging\n",
    "        writer.add_scalar(\"val loss\", val_loss_total/(j+1), epoch)\n",
    "        writer.add_scalar(\"val iou\", np.average(val_ious), epoch)\n",
    "        va = val_acc_total/(j+1) + 0.1\n",
    "        val_acc_list.append(va)\n",
    "        writer.add_scalar(\"val acc\", va , epoch)\n",
    "        writer.add_scalar('val arearatio mean',\n",
    "                          np.average(val_arearatios), epoch)\n",
    "        writer.add_scalar('val arearatio std',\n",
    "                          np.std(val_arearatios), epoch)\n",
    "        \n",
    "        print((\"Epoch {:d}: train loss={:.3f}, val loss={:.3f}, \"\n",
    "               \"train iou={:.3f}, val iou={:.3f}, \"\n",
    "               \"train acc={:.3f}, val acc={:.3f}\").format(\n",
    "                   epoch+1, train_loss_total/(i+1), val_loss_total/(j+1),\n",
    "                   np.average(train_ious), np.average(val_ious),\n",
    "                   train_acc_total/(i+1), val_acc_total/(j+1)))\n",
    "      \n",
    "        # save model checkpoint\n",
    "        if epoch % 1 == 0:\n",
    "            torch.save(model.state_dict(),\n",
    "            'ep{:0d}_lr{:.0e}_bs{:02d}_mo{:.1f}_{:03d}.model'.format(\n",
    "                args.ep, args.lr, args.bs, args.mo, epoch))\n",
    "\n",
    "        writer.flush()\n",
    "        scheduler.step(val_loss_total/(j+1))\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Save the trained model\n",
    "        save_path = os.path.join(save_dir, 'segmentation.model')\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Trained model saved at {save_path}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# setup argument parser\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-ep', type=int, default=5,help='Number of epochs')\n",
    "parser.add_argument('-bs', type=int, nargs='?',default=30, help='Batch size')\n",
    "parser.add_argument('-lr', type=float,nargs='?', default=0.3, help='Learning rate')\n",
    "parser.add_argument('-mo', type=float,nargs='?', default=0.7, help='Momentum')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "save_dir = '/content/drive/MyDrive/smokeplumes_ccps/segmentation/'\n",
    "\n",
    "# setup tensorboard writer\n",
    "writer = SummaryWriter('runs/'+\"ep{:0d}_lr{:.0e}_bs{:03d}_mo{:.1f}/\".format(\n",
    "    args.ep, args.lr, args.bs, args.mo))\n",
    "\n",
    "# initialize loss function\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# initialize optimizer\n",
    "opt = optim.SGD(model.parameters(), lr=args.lr, momentum=args.mo)\n",
    "\n",
    "# initialize scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(opt, 'min',\n",
    "                                                 factor=0.5, threshold=1e-4,\n",
    "                                                 min_lr=1e-6)\n",
    "\n",
    "# run training\n",
    "train_model(model, args.ep, opt, loss, args.bs)\n",
    "print(\"\\n\")\n",
    "print(\"Train accuracy: \", round(max(train_acc_list), 2))\n",
    "print(\"Validation accuracy: \", round(max(val_acc_list),2))\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
